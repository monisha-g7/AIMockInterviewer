{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOuhSqOoSDzy"
   },
   "outputs": [],
   "source": [
    "# Set your API key directly (replace with your actual OpenAI API key)\n",
    "api_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"  # ⚠️ Make sure to secure this or use secrets management in production\n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n4kNJjgASMar"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import random\n",
    "\n",
    "# Set your API key directly\n",
    "api_key = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" \n",
    "\n",
    "# Initialize the OpenAI client with the API key\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bCq5szCoYfCO"
   },
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the RAG evaluator to track metrics related to RAG performance\n",
    "        \"\"\"\n",
    "        self.retrieval_metrics = {\n",
    "            'relevance_scores': [],          # Relevance scores for retrieved questions\n",
    "            'query_similarity_scores': [],   # Query-question similarity scores\n",
    "            'category_matches': [],          # Whether retrieved questions match requested categories\n",
    "            'difficulty_matches': []         # Whether retrieved questions match requested difficulty\n",
    "        }\n",
    "\n",
    "        self.response_metrics = {\n",
    "            'evaluation_quality': [],        # Metric for quality of evaluations\n",
    "            'reference_similarity': [],      # Similarity between user responses and reference answers\n",
    "            'evaluation_time': []            # Time taken to evaluate responses\n",
    "        }\n",
    "\n",
    "        self.user_feedback = {\n",
    "            'question_relevance': [],        # User ratings of question relevance\n",
    "            'evaluation_fairness': [],       # User ratings of evaluation fairness\n",
    "            'overall_satisfaction': None     # Overall user satisfaction with the interview\n",
    "        }\n",
    "\n",
    "    def log_retrieval_metrics(self, query_embedding, retrieved_questions, question_embeddings,\n",
    "                             requested_categories=None, requested_difficulty=None):\n",
    "        \"\"\"\n",
    "        Log metrics related to question retrieval\n",
    "\n",
    "        Args:\n",
    "            query_embedding: The embedding of the query used for retrieval\n",
    "            retrieved_questions: List of retrieved question data\n",
    "            question_embeddings: Matrix of all question embeddings\n",
    "            requested_categories: Categories requested by the user\n",
    "            requested_difficulty: Difficulty level requested by the user\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        import numpy as np\n",
    "\n",
    "        # Get indices of retrieved questions\n",
    "        retrieved_indices = [q['original_index'] for q in retrieved_questions]\n",
    "\n",
    "        # Calculate relevance scores (using cosine similarity)\n",
    "        similarities = cosine_similarity([query_embedding], question_embeddings)[0]\n",
    "        retrieved_similarities = similarities[retrieved_indices]\n",
    "        self.retrieval_metrics['relevance_scores'].extend(retrieved_similarities.tolist())\n",
    "        self.retrieval_metrics['query_similarity_scores'].extend(retrieved_similarities.tolist())\n",
    "\n",
    "        # Log category and difficulty matches if provided\n",
    "        if requested_categories:\n",
    "            for q in retrieved_questions:\n",
    "                category_match = q.get('category', '') in requested_categories\n",
    "                self.retrieval_metrics['category_matches'].append(category_match)\n",
    "\n",
    "        if requested_difficulty:\n",
    "            for q in retrieved_questions:\n",
    "                difficulty_match = q.get('difficulty_level', '') == requested_difficulty\n",
    "                self.retrieval_metrics['difficulty_matches'].append(difficulty_match)\n",
    "\n",
    "    def log_response_evaluation(self, question, user_response, reference_answer, evaluation, score, evaluation_time):\n",
    "        \"\"\"\n",
    "        Log metrics related to response evaluation\n",
    "\n",
    "        Args:\n",
    "            question: The question that was asked\n",
    "            user_response: User's response to the question\n",
    "            reference_answer: Reference answer from the question bank\n",
    "            evaluation: Evaluation text from the LLM\n",
    "            score: Score given by the LLM\n",
    "            evaluation_time: Time taken to evaluate the response\n",
    "        \"\"\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        # Record evaluation time\n",
    "        self.response_metrics['evaluation_time'].append(evaluation_time)\n",
    "\n",
    "        # Calculate similarity between user response and reference answer\n",
    "        try:\n",
    "            encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            user_embedding = encoder.encode([user_response])[0]\n",
    "            ref_embedding = encoder.encode([reference_answer])[0]\n",
    "            similarity = cosine_similarity([user_embedding], [ref_embedding])[0][0]\n",
    "            self.response_metrics['reference_similarity'].append(similarity)\n",
    "\n",
    "            # Evaluate if the evaluation text mentions key points from reference answer\n",
    "            # This is a simple heuristic - could be enhanced with more sophisticated methods\n",
    "            eval_quality = 0.5  # Default quality score\n",
    "            key_terms = self._extract_key_terms(reference_answer)\n",
    "            mentioned_terms = sum(1 for term in key_terms if term.lower() in evaluation.lower())\n",
    "            if key_terms:\n",
    "                eval_quality = mentioned_terms / len(key_terms)\n",
    "            self.response_metrics['evaluation_quality'].append(eval_quality)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating evaluation metrics: {e}\")\n",
    "            self.response_metrics['reference_similarity'].append(0.0)\n",
    "            self.response_metrics['evaluation_quality'].append(0.0)\n",
    "\n",
    "    def _extract_key_terms(self, text, max_terms=5):\n",
    "        \"\"\"Extract key terms from a text using a simple frequency-based approach\"\"\"\n",
    "        import re\n",
    "        from collections import Counter\n",
    "\n",
    "        # Remove common words and punctuation\n",
    "        words = re.findall(r'\\b[A-Za-z]{3,}\\b', text.lower())\n",
    "        stop_words = {'the', 'and', 'is', 'in', 'to', 'of', 'that', 'for', 'are', 'with', 'as', 'can', 'be', 'this', 'an', 'or'}\n",
    "        filtered_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "        # Return the most common words\n",
    "        return [word for word, _ in Counter(filtered_words).most_common(max_terms)]\n",
    "\n",
    "    def collect_user_feedback(self, question_idx, relevance_rating=None, fairness_rating=None):\n",
    "        \"\"\"\n",
    "        Collect user feedback on question relevance and evaluation fairness\n",
    "\n",
    "        Args:\n",
    "            question_idx: Index of the question being rated\n",
    "            relevance_rating: User rating of question relevance (1-5)\n",
    "            fairness_rating: User rating of evaluation fairness (1-5)\n",
    "        \"\"\"\n",
    "        if relevance_rating is not None:\n",
    "            self.user_feedback['question_relevance'].append((question_idx, relevance_rating))\n",
    "\n",
    "        if fairness_rating is not None:\n",
    "            self.user_feedback['evaluation_fairness'].append((question_idx, fairness_rating))\n",
    "\n",
    "    def set_overall_satisfaction(self, rating):\n",
    "        \"\"\"Set overall satisfaction rating from the user\"\"\"\n",
    "        self.user_feedback['overall_satisfaction'] = rating\n",
    "\n",
    "    def generate_evaluation_report(self):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive evaluation report for the RAG system\n",
    "\n",
    "        Returns:\n",
    "            Formatted report as a string\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "\n",
    "        report = \"# RAG System Evaluation Report\\n\\n\"\n",
    "\n",
    "        # Retrieval metrics\n",
    "        report += \"## Retrieval Performance\\n\\n\"\n",
    "\n",
    "        if self.retrieval_metrics['relevance_scores']:\n",
    "            avg_relevance = np.mean(self.retrieval_metrics['relevance_scores'])\n",
    "            report += f\"- Average Relevance Score: {avg_relevance:.3f}\\n\"\n",
    "\n",
    "        if self.retrieval_metrics['category_matches']:\n",
    "            category_match_rate = np.mean(self.retrieval_metrics['category_matches'])\n",
    "            report += f\"- Category Match Rate: {category_match_rate:.2%}\\n\"\n",
    "\n",
    "        if self.retrieval_metrics['difficulty_matches']:\n",
    "            difficulty_match_rate = np.mean(self.retrieval_metrics['difficulty_matches'])\n",
    "            report += f\"- Difficulty Match Rate: {difficulty_match_rate:.2%}\\n\"\n",
    "\n",
    "        # Response evaluation metrics\n",
    "        report += \"\\n## Response Evaluation Performance\\n\\n\"\n",
    "\n",
    "        if self.response_metrics['evaluation_time']:\n",
    "            avg_eval_time = np.mean(self.response_metrics['evaluation_time'])\n",
    "            report += f\"- Average Evaluation Time: {avg_eval_time:.2f} seconds\\n\"\n",
    "\n",
    "        if self.response_metrics['evaluation_quality']:\n",
    "            avg_quality = np.mean(self.response_metrics['evaluation_quality'])\n",
    "            report += f\"- Evaluation Quality Score: {avg_quality:.2f}\\n\"\n",
    "\n",
    "        if self.response_metrics['reference_similarity']:\n",
    "            avg_similarity = np.mean(self.response_metrics['reference_similarity'])\n",
    "            report += f\"- Average Response-Reference Similarity: {avg_similarity:.3f}\\n\"\n",
    "\n",
    "        # User feedback\n",
    "        report += \"\\n## User Feedback\\n\\n\"\n",
    "\n",
    "        if self.user_feedback['question_relevance']:\n",
    "            ratings = [r for _, r in self.user_feedback['question_relevance']]\n",
    "            avg_relevance = np.mean(ratings)\n",
    "            report += f\"- Average Question Relevance Rating: {avg_relevance:.2f}/5\\n\"\n",
    "\n",
    "        if self.user_feedback['evaluation_fairness']:\n",
    "            ratings = [r for _, r in self.user_feedback['evaluation_fairness']]\n",
    "            avg_fairness = np.mean(ratings)\n",
    "            report += f\"- Average Evaluation Fairness Rating: {avg_fairness:.2f}/5\\n\"\n",
    "\n",
    "        if self.user_feedback['overall_satisfaction'] is not None:\n",
    "            report += f\"- Overall User Satisfaction: {self.user_feedback['overall_satisfaction']}/5\\n\"\n",
    "\n",
    "        # Overall assessment\n",
    "        report += \"\\n## Overall Assessment\\n\\n\"\n",
    "\n",
    "        # Calculate overall retrieval score\n",
    "        retrieval_score = 0\n",
    "        if self.retrieval_metrics['relevance_scores']:\n",
    "            retrieval_score = np.mean(self.retrieval_metrics['relevance_scores'])\n",
    "\n",
    "        # Calculate overall evaluation score\n",
    "        eval_score = 0\n",
    "        if self.response_metrics['evaluation_quality']:\n",
    "            eval_score = np.mean(self.response_metrics['evaluation_quality'])\n",
    "\n",
    "        # Calculate overall user satisfaction\n",
    "        user_score = 0\n",
    "        if self.user_feedback['overall_satisfaction'] is not None:\n",
    "            user_score = self.user_feedback['overall_satisfaction'] / 5\n",
    "        elif self.user_feedback['question_relevance']:\n",
    "            ratings = [r for _, r in self.user_feedback['question_relevance']]\n",
    "            user_score = np.mean(ratings) / 5\n",
    "\n",
    "        # Calculate overall RAG quality score\n",
    "        rag_quality = (retrieval_score * 0.4) + (eval_score * 0.3) + (user_score * 0.3)\n",
    "        report += f\"- Overall RAG Quality Score: {rag_quality:.2f} (0-1 scale)\\n\"\n",
    "\n",
    "        # Add recommendations based on scores\n",
    "        report += \"\\n## Recommendations\\n\\n\"\n",
    "\n",
    "        if retrieval_score < 0.7:\n",
    "            report += \"- Consider improving retrieval by refining embeddings or using hybrid retrieval\\n\"\n",
    "\n",
    "        if eval_score < 0.7:\n",
    "            report += \"- Enhance evaluation prompts to better assess user responses against reference answers\\n\"\n",
    "\n",
    "        if user_score < 0.7:\n",
    "            report += \"- Focus on improving user experience and question relevance\\n\"\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "62p6iiJ3SMYf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "class MockInterviewer:\n",
    "    def __init__(self, qb_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the mock interviewer with the question bank\n",
    "\n",
    "        Args:\n",
    "            qb_path: Path to the CSV file containing the question bank\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.question_bank = pd.read_csv(qb_path)\n",
    "            print(f\"Loaded question bank with {len(self.question_bank)} questions.\")\n",
    "\n",
    "            # Print first few rows and columns to verify data\n",
    "            print(\"\\nFirst few rows of the question bank:\")\n",
    "            print(self.question_bank.head(2))\n",
    "\n",
    "            # Check if required columns exist\n",
    "            required_columns = ['questions', 'category', 'company', 'difficulty_level', 'Answer']\n",
    "            for col in required_columns:\n",
    "                if col not in self.question_bank.columns:\n",
    "                    print(f\"Warning: Required column '{col}' not found in the question bank.\")\n",
    "                    if col == 'questions' and 'question' in self.question_bank.columns:\n",
    "                        print(f\"Using 'question' column instead of 'questions'\")\n",
    "                        self.question_bank['questions'] = self.question_bank['question']\n",
    "                    elif col == 'Answer' and 'answer' in self.question_bank.columns:\n",
    "                        print(f\"Using 'answer' column instead of 'Answer'\")\n",
    "                        self.question_bank['Answer'] = self.question_bank['answer']\n",
    "\n",
    "            # Initialize the sentence transformer model for embeddings\n",
    "            print(\"\\nLoading the sentence transformer model...\")\n",
    "            self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            print(\"Model loaded successfully!\")\n",
    "\n",
    "            # Get the questions column (could be 'questions' or 'question')\n",
    "            questions_col = 'questions' if 'questions' in self.question_bank.columns else 'question'\n",
    "\n",
    "            # Precompute embeddings for all questions in the bank\n",
    "            print(\"Computing embeddings for questions...\")\n",
    "            self.question_embeddings = self.encoder.encode(self.question_bank[questions_col].tolist())\n",
    "            print(f\"Generated {len(self.question_embeddings)} embeddings.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading question bank: {e}\")\n",
    "            print(\"Creating sample question bank as fallback...\")\n",
    "            self.question_bank = self.create_sample_qb()\n",
    "            print(\"Initializing embeddings with sample data...\")\n",
    "            self.encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self.question_embeddings = self.encoder.encode(self.question_bank['questions'].tolist())\n",
    "\n",
    "        # Added for RAG Evaluation\n",
    "        # Initialize the RAG evaluator\n",
    "        self.evaluator = RAGEvaluator()\n",
    "\n",
    "        # Track original indices in the question bank\n",
    "        self.question_bank.reset_index(inplace=True)\n",
    "        self.question_bank.rename(columns={'index': 'original_index'}, inplace=True)\n",
    "        # Added for RAG Evaluation\n",
    "\n",
    "        # Initialize dictionaries to store results\n",
    "        self.responses = {}\n",
    "        self.evaluations = {}\n",
    "        self.scores = {}\n",
    "        # User profile\n",
    "        self.user_profile = {}\n",
    "\n",
    "    def create_sample_qb(self):\n",
    "        \"\"\"Create a sample question bank for demonstration\"\"\"\n",
    "        print(\"Creating a sample question bank for demonstration...\")\n",
    "\n",
    "        sample_data = {\n",
    "            \"questions\": [\n",
    "                \"Explain the difference between supervised and unsupervised learning.\",\n",
    "                \"What is the purpose of regularization in machine learning?\",\n",
    "                \"Explain the concept of backpropagation in neural networks.\",\n",
    "                \"What is the CAP theorem in distributed systems?\",\n",
    "                \"How would you implement a recommendation system for an e-commerce platform?\",\n",
    "                \"Explain the concept of REST API and its principles.\",\n",
    "                \"What are microservices and what are their advantages?\",\n",
    "                \"How do you handle concurrency issues in a distributed system?\",\n",
    "                \"Explain the concept of Docker and containerization.\",\n",
    "                \"How would you design a URL shortening service?\"\n",
    "            ],\n",
    "            \"category\": [\"ML\", \"ML\", \"ML\", \"System Design\", \"ML\", \"Web Development\", \"System Design\", \"System Design\", \"DevOps\", \"System Design\"],\n",
    "            \"company\": [\"Google\", \"Meta\", \"Amazon\", \"Microsoft\", \"Netflix\", \"Amazon\", \"Google\", \"Meta\", \"Microsoft\", \"Netflix\"],\n",
    "            \"difficulty_level\": [\"Medium\", \"Medium\", \"Hard\", \"Hard\", \"Hard\", \"Easy\", \"Medium\", \"Hard\", \"Medium\", \"Medium\"],\n",
    "            \"Answer\": [\n",
    "                \"Supervised learning uses labeled data to train models, where the algorithm learns to map inputs to known outputs. Unsupervised learning works with unlabeled data to find patterns or structures without predefined outputs.\",\n",
    "                \"Regularization prevents overfitting by adding a penalty term to the loss function, discouraging complex models. Common techniques include L1 (Lasso) and L2 (Ridge) regularization.\",\n",
    "                \"Backpropagation is an algorithm for training neural networks that calculates gradients of the loss function with respect to weights using the chain rule, propagating errors backward from output to input layers.\",\n",
    "                \"The CAP theorem states that a distributed system cannot simultaneously provide Consistency, Availability, and Partition tolerance. You must choose two out of three properties.\",\n",
    "                \"A recommendation system for e-commerce would use collaborative filtering, content-based filtering, or hybrid approaches. It would analyze user behavior, purchase history, and item similarities to suggest relevant products.\",\n",
    "                \"REST (Representational State Transfer) is an architectural style for web services. Its principles include client-server architecture, statelessness, cacheability, uniform interface, layered system, and code on demand.\",\n",
    "                \"Microservices are an architectural style where applications are built as small, independent services. Advantages include scalability, technology diversity, resilience, and easier deployment.\",\n",
    "                \"Concurrency issues in distributed systems can be handled using locks, distributed transactions, eventual consistency models, conflict resolution strategies, and specialized data structures like CRDTs.\",\n",
    "                \"Docker is a platform for developing, shipping, and running applications in containers. Containerization packages code and dependencies together, ensuring consistent operation across environments.\",\n",
    "                \"A URL shortening service design includes components for URL shortening (hash function), storage (database), redirection service, analytics, and scaling considerations like caching and load balancing.\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        return pd.DataFrame(sample_data)\n",
    "\n",
    "    def get_user_profile(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get user profile information to personalize the interview\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing user profile information\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Welcome to the Mock Interview App ---\\n\")\n",
    "        years_exp = input(\"How many years of experience do you have? \")\n",
    "        job_role = input(\"What job role are you interviewing for? \")\n",
    "        skills = input(\"List your key skills (comma separated): \")\n",
    "\n",
    "        self.user_profile = {\n",
    "            \"years_of_experience\": years_exp,\n",
    "            \"job_role\": job_role,\n",
    "            \"skills\": [skill.strip() for skill in skills.split(\",\")]\n",
    "        }\n",
    "\n",
    "        print(f\"\\nThanks! I'll prepare an interview for a {job_role} position with {years_exp} years of experience.\\n\")\n",
    "        return self.user_profile\n",
    "\n",
    "    def retrieve_relevant_questions(self, n: int = 5, categories: Optional[List[str]] = None,\n",
    "                               difficulty: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant questions based on user profile\n",
    "\n",
    "        Args:\n",
    "            n: Number of questions to retrieve\n",
    "            categories: Optional list of categories to filter by\n",
    "            difficulty: Optional difficulty level to filter by\n",
    "\n",
    "        Returns:\n",
    "            List of relevant questions\n",
    "        \"\"\"\n",
    "        # Create a query based on user profile\n",
    "        query = f\"Interview questions for {self.user_profile['job_role']} position with {self.user_profile['years_of_experience']} years of experience in {', '.join(self.user_profile['skills'])}\"\n",
    "        if categories:\n",
    "            query += f\" focusing on {', '.join(categories)}\"\n",
    "        if difficulty:\n",
    "            query += f\" at {difficulty} difficulty level\"\n",
    "\n",
    "        print(f\"Query for question selection: {query}\")\n",
    "\n",
    "        # Get query embedding\n",
    "        query_embedding = self.encoder.encode([query])[0]\n",
    "\n",
    "        # Calculate similarity scores\n",
    "        similarities = cosine_similarity([query_embedding], self.question_embeddings)[0]\n",
    "\n",
    "        # Apply category filter if specified\n",
    "        filtered_indices = list(range(len(self.question_bank)))\n",
    "        if categories:\n",
    "            filtered_indices = [i for i, cat in enumerate(self.question_bank['category'])\n",
    "                              if cat in categories]\n",
    "\n",
    "        # Apply difficulty filter if specified\n",
    "        if difficulty and filtered_indices:\n",
    "            filtered_indices = [i for i in filtered_indices\n",
    "                              if self.question_bank.iloc[i]['difficulty_level'] == difficulty]\n",
    "\n",
    "        # If we have no matches after filtering, use all questions\n",
    "        if not filtered_indices:\n",
    "            filtered_indices = list(range(len(self.question_bank)))\n",
    "            print(\"No matches found with filters. Using all questions.\")\n",
    "\n",
    "        # Get similarities only for filtered indices\n",
    "        filtered_similarities = [(i, similarities[i]) for i in filtered_indices]\n",
    "\n",
    "        # Sort by similarity\n",
    "        filtered_similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Get top n indices\n",
    "        top_indices = [i for i, _ in filtered_similarities[:n]]\n",
    "\n",
    "        # Get the questions\n",
    "        selected_questions = []\n",
    "        questions_col = 'questions' if 'questions' in self.question_bank.columns else 'question'\n",
    "\n",
    "        for idx in top_indices:\n",
    "            question_data = self.question_bank.iloc[idx].to_dict()\n",
    "            # Make sure we have all required fields\n",
    "            if 'questions' not in question_data and questions_col in self.question_bank.columns:\n",
    "                question_data['questions'] = question_data[questions_col]\n",
    "            selected_questions.append(question_data)\n",
    "            print(f\"Selected question with similarity score {similarities[idx]:.4f}: {question_data['questions'][:50]}...\")\n",
    "\n",
    "        # Log retrieval metrics\n",
    "        self.evaluator.log_retrieval_metrics(\n",
    "            query_embedding=query_embedding,\n",
    "            retrieved_questions=selected_questions,\n",
    "            question_embeddings=self.question_embeddings,\n",
    "            requested_categories=categories,\n",
    "            requested_difficulty=difficulty\n",
    "        )\n",
    "\n",
    "        return selected_questions\n",
    "\n",
    "    def conduct_interview(self, num_questions: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Conduct the interview using selected questions\n",
    "\n",
    "        Args:\n",
    "            num_questions: Number of questions to ask\n",
    "        \"\"\"\n",
    "        if not self.user_profile:\n",
    "            self.get_user_profile()\n",
    "\n",
    "        # Get category and difficulty preferences\n",
    "        print(\"\\nWould you like to focus on specific categories?\")\n",
    "        print(\"Available categories: ML, System Design, Web Development, DevOps, or leave blank for all\")\n",
    "        category_input = input(\"Enter categories (comma-separated) or press Enter for all: \")\n",
    "        categories = [cat.strip() for cat in category_input.split(\",\")] if category_input.strip() else None\n",
    "\n",
    "        print(\"\\nWould you like to focus on a specific difficulty level?\")\n",
    "        print(\"Available difficulties: Easy, Medium, Hard, or leave blank for all\")\n",
    "        difficulty = input(\"Enter difficulty or press Enter for all: \").strip() or None\n",
    "\n",
    "        print(f\"\\nRetrieving {num_questions} relevant questions based on your profile...\")\n",
    "        relevant_questions = self.retrieve_relevant_questions(\n",
    "            num_questions,\n",
    "            categories=categories if categories and categories[0] else None,\n",
    "            difficulty=difficulty\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Starting Interview ---\\n\")\n",
    "        print(f\"I'll ask you {num_questions} questions relevant to your profile. Please provide detailed answers.\\n\")\n",
    "\n",
    "        for i, question_data in enumerate(relevant_questions):\n",
    "            question = question_data['questions']\n",
    "            category = question_data.get('category', 'General')\n",
    "            difficulty = question_data.get('difficulty_level', 'Medium')\n",
    "\n",
    "            # Handle different case variations for the Answer field\n",
    "            reference_answer = None\n",
    "            for key in ['Answer', 'answer', 'ANSWER']:\n",
    "                if key in question_data:\n",
    "                    reference_answer = question_data[key]\n",
    "                    break\n",
    "\n",
    "            if reference_answer is None:\n",
    "                reference_answer = \"No reference answer provided.\"\n",
    "\n",
    "            print(f\"\\nQuestion {i+1} ({category}, Difficulty: {difficulty}):\")\n",
    "            print(question)\n",
    "\n",
    "            # Get user's response\n",
    "            response = input(\"\\nYour answer: \")\n",
    "            self.responses[question] = response\n",
    "\n",
    "            print(\"\\nEvaluating your response...\")\n",
    "            # Evaluate response\n",
    "            evaluation, score = self.evaluate_response(question, response, reference_answer)\n",
    "            self.evaluations[question] = evaluation\n",
    "            self.scores[question] = score\n",
    "\n",
    "            # Provide immediate feedback\n",
    "            print(f\"\\nEvaluation: {evaluation}\")\n",
    "            print(f\"Score: {score}/10\")\n",
    "\n",
    "            # Collect user feedback on question relevance\n",
    "            relevance_rating = input(\"\\nHow relevant was this question to your job role? (1-5, 5 being most relevant): \")\n",
    "            try:\n",
    "                relevance_rating = int(relevance_rating)\n",
    "                if 1 <= relevance_rating <= 5:\n",
    "                    self.evaluator.collect_user_feedback(i, relevance_rating=relevance_rating)\n",
    "                else:\n",
    "                    print(\"Invalid rating. Skipping feedback.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid rating. Skipping feedback.\")\n",
    "\n",
    "            # Collect user feedback on evaluation fairness\n",
    "            fairness_rating = input(\"How fair was this evaluation? (1-5, 5 being most fair): \")\n",
    "            try:\n",
    "                fairness_rating = int(fairness_rating)\n",
    "                if 1 <= fairness_rating <= 5:\n",
    "                    self.evaluator.collect_user_feedback(i, fairness_rating=fairness_rating)\n",
    "                else:\n",
    "                    print(\"Invalid rating. Skipping feedback.\")\n",
    "            except ValueError:\n",
    "                print(\"Invalid rating. Skipping feedback.\")\n",
    "\n",
    "        # Collect overall satisfaction rating\n",
    "        overall_rating = input(\"\\nOverall, how satisfied are you with this interview experience? (1-5, 5 being most satisfied): \")\n",
    "        try:\n",
    "            overall_rating = int(overall_rating)\n",
    "            if 1 <= overall_rating <= 5:\n",
    "                self.evaluator.set_overall_satisfaction(overall_rating)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Generate summary at the end\n",
    "        self.generate_summary()\n",
    "\n",
    "        # Generate RAG evaluation report\n",
    "        self.generate_rag_report()\n",
    "\n",
    "    def evaluate_response(self, question: str, response: str, reference: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate user's response using GPT-3.5\n",
    "\n",
    "        Args:\n",
    "            question: The question asked\n",
    "            response: User's response\n",
    "            reference: Reference answer from the question bank\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (evaluation text, score)\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            prompt = \"\"\"\n",
    "            You are evaluating a candidate's response in a technical interview.\n",
    "\n",
    "            Question: {}\n",
    "\n",
    "            Reference Answer: {}\n",
    "\n",
    "            Candidate's Response: {}\n",
    "\n",
    "            Please evaluate the candidate's response considering the following:\n",
    "            1. Correctness and accuracy of information\n",
    "            2. Completeness of the answer\n",
    "            3. Clarity and communication\n",
    "            4. Technical depth and understanding\n",
    "\n",
    "            Provide a brief evaluation (2-3 sentences) and a score out of 10.\n",
    "\n",
    "            Format your response as:\n",
    "            Evaluation: [Your evaluation here]\n",
    "            Score: [Score]/10\n",
    "            \"\"\".format(question, reference, response)\n",
    "\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "\n",
    "            result = completion.choices[0].message.content\n",
    "\n",
    "            # Extract evaluation and score with enhanced error handling\n",
    "            try:\n",
    "                if \"Evaluation:\" in result and \"Score:\" in result:\n",
    "                    evaluation_part = result.split(\"Evaluation:\")[1].split(\"Score:\")[0].strip()\n",
    "                    score_part = result.split(\"Score:\")[1].strip()\n",
    "                    score = float(score_part.split(\"/\")[0])\n",
    "                else:\n",
    "                    # Fallback parsing if format is not exactly as expected\n",
    "                    lines = result.split(\"\\n\")\n",
    "                    evaluation_part = next((line for line in lines if \"valuation\" in line), \"\")\n",
    "                    evaluation_part = evaluation_part.split(\":\", 1)[1].strip() if \":\" in evaluation_part else \"\"\n",
    "\n",
    "                    score_line = next((line for line in lines if \"core\" in line), \"\")\n",
    "                    score_text = score_line.split(\":\", 1)[1].strip() if \":\" in score_line else \"\"\n",
    "                    score = float(score_text.split(\"/\")[0]) if \"/\" in score_text else 5.0\n",
    "\n",
    "                    if not evaluation_part:\n",
    "                        evaluation_part = \"Response evaluated based on correctness, completeness, clarity, and technical depth.\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing evaluation: {e}\")\n",
    "                evaluation_part = \"Unable to parse evaluation correctly.\"\n",
    "                score = 5.0\n",
    "\n",
    "            # Calculate time taken\n",
    "            evaluation_time = time.time() - start_time\n",
    "\n",
    "            # Log evaluation metrics\n",
    "            self.evaluator.log_response_evaluation(\n",
    "                question=question,\n",
    "                user_response=response,\n",
    "                reference_answer=reference,\n",
    "                evaluation=evaluation_part,\n",
    "                score=score,\n",
    "                evaluation_time=evaluation_time\n",
    "            )\n",
    "\n",
    "            return evaluation_part, score\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in evaluation: {e}\")\n",
    "            evaluation_time = time.time() - start_time\n",
    "            # Log failed evaluation\n",
    "            self.evaluator.log_response_evaluation(\n",
    "                question=question,\n",
    "                user_response=response,\n",
    "                reference_answer=reference,\n",
    "                evaluation=\"Unable to evaluate due to an error.\",\n",
    "                score=5.0,\n",
    "                evaluation_time=evaluation_time\n",
    "            )\n",
    "            return \"Unable to evaluate due to an error.\", 5.0\n",
    "\n",
    "    def generate_summary(self) -> str:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive summary of the interview\n",
    "\n",
    "        Returns:\n",
    "            Summary text\n",
    "        \"\"\"\n",
    "        print(\"\\nGenerating interview summary...\")\n",
    "        # Prepare data for summary\n",
    "        questions_and_scores = []\n",
    "        for question, score in self.scores.items():\n",
    "            questions_and_scores.append(f\"Question: {question}\\nScore: {score}/10\")\n",
    "\n",
    "        avg_score = sum(self.scores.values()) / len(self.scores)\n",
    "\n",
    "        # Generate summary using GPT-3.5\n",
    "        try:\n",
    "            # Using string formatting instead of f-strings to avoid backslash issues\n",
    "            prompt = \"\"\"\n",
    "            You are an interview coach. Please generate a comprehensive summary of this technical interview.\n",
    "\n",
    "            Candidate Profile:\n",
    "            - Job Role: {}\n",
    "            - Experience: {} years\n",
    "            - Skills: {}\n",
    "\n",
    "            Interview Results:\n",
    "            {}\n",
    "\n",
    "            Average Score: {:.1f}/10\n",
    "\n",
    "            Please provide:\n",
    "            1. Overall performance assessment\n",
    "            2. Key strengths (at least 3)\n",
    "            3. Areas for improvement (at least 3)\n",
    "            4. Recommendations for next steps\n",
    "\n",
    "            Format your response in a clear, structured way with sections.\n",
    "            \"\"\".format(\n",
    "                self.user_profile['job_role'],\n",
    "                self.user_profile['years_of_experience'],\n",
    "                ', '.join(self.user_profile['skills']),\n",
    "                '\\n\\n'.join(questions_and_scores),\n",
    "                avg_score\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "\n",
    "                summary = completion.choices[0].message.content\n",
    "            except Exception as api_error:\n",
    "                print(f\"Error with API call: {api_error}\")\n",
    "                # Create a simplified summary without API call as fallback\n",
    "                summary = \"\"\"\n",
    "                # INTERVIEW SUMMARY\n",
    "\n",
    "                ## Overall Performance Assessment\n",
    "                Your overall score was {:.1f}/10.\n",
    "\n",
    "                ## Key Strengths\n",
    "                - You completed the interview process\n",
    "                - You provided answers to all questions\n",
    "                - You demonstrated knowledge in your field\n",
    "\n",
    "                ## Areas for Improvement\n",
    "                - Work on providing more detailed answers\n",
    "                - Include more examples in your responses\n",
    "                - Connect theoretical knowledge with practical applications\n",
    "\n",
    "                ## Recommendations\n",
    "                - Review the technical areas covered in this interview\n",
    "                - Practice explaining complex concepts simply\n",
    "                - Prepare examples from your experience for common interview questions\n",
    "                \"\"\".format(avg_score)\n",
    "\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"INTERVIEW SUMMARY\")\n",
    "            print(\"=\"*50)\n",
    "            print(summary)\n",
    "\n",
    "            return summary\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating summary: {e}\")\n",
    "            return \"Unable to generate summary due to an error.\"\n",
    "\n",
    "        # Add a new method to generate the RAG evaluation report\n",
    "    def generate_rag_report(self) -> None:\n",
    "        \"\"\"Generate and display the RAG evaluation report\"\"\"\n",
    "        report = self.evaluator.generate_evaluation_report()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RAG EVALUATION REPORT\")\n",
    "        print(\"=\"*50)\n",
    "        print(report)\n",
    "\n",
    "        # Ask if user wants to save the report\n",
    "        save_report = input(\"\\nWould you like to save this report to a file? (y/n): \")\n",
    "        if save_report.lower() == 'y':\n",
    "            filename = input(\"Enter filename (default: rag_evaluation_report.md): \") or \"rag_evaluation_report.md\"\n",
    "            try:\n",
    "                with open(filename, 'w') as f:\n",
    "                    f.write(report)\n",
    "                print(f\"Report saved to {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving report: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0Qty6KpSMVg",
    "outputId": "91395aa0-8bc9-44a9-81c6-05535be0c41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock Interviewer App Started\n",
      "Loaded question bank with 3346 questions.\n",
      "\n",
      "First few rows of the question bank:\n",
      "                                           questions           category  \\\n",
      "0  \"Can you describe a time when you used data to...  Analytical Skills   \n",
      "1  \"How do you prioritize tasks when analyzing la...  Analytical Skills   \n",
      "\n",
      "  company difficulty_level                                             Answer  \n",
      "0   Other           medium  One time I used data to make a decision was wh...  \n",
      "1   Other           medium  When analyzing large volumes of data, it is im...  \n",
      "\n",
      "Loading the sentence transformer model...\n",
      "Model loaded successfully!\n",
      "Computing embeddings for questions...\n",
      "Generated 3346 embeddings.\n",
      "\n",
      "--- Welcome to the Mock Interview App ---\n",
      "\n",
      "How many years of experience do you have? 4\n",
      "What job role are you interviewing for? Data Analyst\n",
      "List your key skills (comma separated): SQL\n",
      "\n",
      "Thanks! I'll prepare an interview for a Data Analyst position with 4 years of experience.\n",
      "\n",
      "\n",
      "How many questions would you like in this interview? (recommended: 3-5): 3\n",
      "\n",
      "Would you like to focus on specific categories?\n",
      "Available categories: ML, System Design, Web Development, DevOps, or leave blank for all\n",
      "Enter categories (comma-separated) or press Enter for all: \n",
      "\n",
      "Would you like to focus on a specific difficulty level?\n",
      "Available difficulties: Easy, Medium, Hard, or leave blank for all\n",
      "Enter difficulty or press Enter for all: Easy\n",
      "\n",
      "Retrieving 3 relevant questions based on your profile...\n",
      "Query for question selection: Interview questions for Data Analyst position with 4 years of experience in SQL at Easy difficulty level\n",
      "No matches found with filters. Using all questions.\n",
      "Selected question with similarity score 0.5968: \"Describe your experience with SQL and its role in...\n",
      "Selected question with similarity score 0.5364: \"Can you explain the steps you take to analyze and...\n",
      "Selected question with similarity score 0.5215: \"What are some common problems you have encountere...\n",
      "\n",
      "--- Starting Interview ---\n",
      "\n",
      "I'll ask you 3 questions relevant to your profile. Please provide detailed answers.\n",
      "\n",
      "\n",
      "Question 1 (Data Analysis, Difficulty: medium):\n",
      "\"Describe your experience with SQL and its role in data analysis.\"\n",
      "\n",
      "Your answer: My experience with SQL has been instrumental in performing effective data analysis. SQL, or Structured Query Language, enables me to retrieve, manipulate, and analyze data stored in relational databases efficiently. I have used SQL to execute complex queries that filter, aggregate, and transform data to extract meaningful insights. Additionally, SQL's role in data analysis is pivotal, as it facilitates the organization of large datasets and supports relational database operations that are essential for data-driven decision-making. Its versatility allows it to be used in various analytical contexts, from simple data retrieval to complex data modeling, making it an invaluable tool in my data analysis toolkit.\n",
      "\n",
      "Evaluating your response...\n",
      "\n",
      "Evaluation: The candidate's response demonstrates a strong understanding of SQL and its role in data analysis. They accurately explain how SQL is used to retrieve and manipulate data, as well as its importance in organizing large datasets for decision-making. The response is clear and effectively communicates the candidate's experience with SQL.\n",
      "Score: 9.0/10\n",
      "\n",
      "How relevant was this question to your job role? (1-5, 5 being most relevant): 5\n",
      "How fair was this evaluation? (1-5, 5 being most fair): 5\n",
      "\n",
      "Question 2 (SQL Server, Difficulty: medium):\n",
      "\"Can you explain the steps you take to analyze and optimize a query in SQL Server?\"\n",
      "\n",
      "Your answer: To optimize a SQL Server query, first analyze the execution plan to identify bottlenecks like table scans or missing indexes. Simplify joins and conditions, and verify that appropriate indexes are utilized. Adjust the database schema if needed, and partition large tables to enhance access speed. Test the optimized query for accuracy and efficiency, monitoring resource usage to prevent unnecessary consumption. Document your changes and their impacts for future reference and continued improvement.\n",
      "\n",
      "Evaluating your response...\n",
      "\n",
      "Evaluation: The candidate's response demonstrates a good understanding of the steps involved in optimizing a SQL Server query. They mention analyzing the execution plan, simplifying joins and conditions, utilizing appropriate indexes, adjusting the schema, partitioning tables, testing the query, monitoring resource usage, and documenting changes. However, they could provide more details on specific tools like SQL Server Profiler and query hints.\n",
      "Score: 8.0/10\n",
      "\n",
      "How relevant was this question to your job role? (1-5, 5 being most relevant): 4\n",
      "How fair was this evaluation? (1-5, 5 being most fair): 3\n",
      "\n",
      "Question 3 (Data Analysis, Difficulty: medium):\n",
      "\"What are some common problems you have encountered during data analysis, and how did you solve them?\"\n",
      "\n",
      "Your answer: During data analysis, I've often encountered issues like missing data, data inconsistency, and outliers. To handle missing data, I use imputation techniques or exclude incomplete records if appropriate. For data inconsistency, I perform data cleaning to standardize formats and correct errors. Outliers are addressed by verifying their authenticity and employing statistical methods to mitigate their impact. These steps ensure data quality and lead to more accurate analysis results.\n",
      "\n",
      "Evaluating your response...\n",
      "\n",
      "Evaluation: The candidate provides a concise and accurate response to common problems encountered during data analysis, such as missing data, data inconsistency, and outliers. They mention appropriate solutions like imputation techniques, data cleaning, and outlier verification. However, more detail on addressing bias in the data could have been included to improve completeness.\n",
      "Score: 8.0/10\n",
      "\n",
      "How relevant was this question to your job role? (1-5, 5 being most relevant): 5\n",
      "How fair was this evaluation? (1-5, 5 being most fair): 4\n",
      "\n",
      "Overall, how satisfied are you with this interview experience? (1-5, 5 being most satisfied): 5\n",
      "\n",
      "Generating interview summary...\n",
      "\n",
      "==================================================\n",
      "INTERVIEW SUMMARY\n",
      "==================================================\n",
      "Overall Performance Assessment:\n",
      "The candidate, with 4 years of experience as a Data Analyst and a strong skill set in SQL, performed well in the technical interview with an average score of 8.3/10. They demonstrated a good understanding of SQL and its role in data analysis, as well as the steps involved in optimizing queries in SQL Server. The candidate also showcased problem-solving skills in addressing common issues encountered during data analysis.\n",
      "\n",
      "Key Strengths:\n",
      "1. Strong SQL Skills: The candidate's proficiency in SQL stood out during the interview, evident in their ability to explain their experience with SQL and the steps taken to analyze and optimize queries.\n",
      "2. Problem-Solving Skills: The candidate effectively addressed common problems encountered in data analysis, showcasing their ability to think critically and troubleshoot issues.\n",
      "3. Communication Skills: The candidate was able to articulate their thoughts and experiences clearly during the interview, demonstrating strong communication skills that are essential for a Data Analyst role.\n",
      "\n",
      "Areas for Improvement:\n",
      "1. Depth of Experience: While the candidate has 4 years of experience as a Data Analyst, delving into more complex scenarios or projects during the interview could further showcase their depth of experience.\n",
      "2. Technical Knowledge Beyond SQL: Expanding knowledge beyond SQL, such as experience with other data analysis tools or techniques, could broaden the candidate's skill set and make them more versatile in the field.\n",
      "3. Confidence in Responses: Building confidence in responses to technical questions can further strengthen the candidate's overall performance, showcasing assurance in their knowledge and abilities.\n",
      "\n",
      "Recommendations for Next Steps:\n",
      "1. Continued Learning: Engage in professional development opportunities to further enhance technical skills and stay updated on industry trends and advancements in data analysis.\n",
      "2. Practice Scenarios: Regular practice with complex data analysis scenarios and problem-solving exercises can help the candidate refine their skills and build confidence in handling diverse challenges.\n",
      "3. Seek Feedback: Request feedback from peers or mentors on interview performance to identify areas for improvement and work towards enhancing overall presentation and communication during technical discussions.\n",
      "\n",
      "==================================================\n",
      "RAG EVALUATION REPORT\n",
      "==================================================\n",
      "# RAG System Evaluation Report\n",
      "\n",
      "## Retrieval Performance\n",
      "\n",
      "- Average Relevance Score: 0.552\n",
      "- Difficulty Match Rate: 0.00%\n",
      "\n",
      "## Response Evaluation Performance\n",
      "\n",
      "- Average Evaluation Time: 1.23 seconds\n",
      "- Evaluation Quality Score: 0.73\n",
      "- Average Response-Reference Similarity: 0.802\n",
      "\n",
      "## User Feedback\n",
      "\n",
      "- Average Question Relevance Rating: 4.67/5\n",
      "- Average Evaluation Fairness Rating: 4.00/5\n",
      "- Overall User Satisfaction: 5/5\n",
      "\n",
      "## Overall Assessment\n",
      "\n",
      "- Overall RAG Quality Score: 0.74 (0-1 scale)\n",
      "\n",
      "## Recommendations\n",
      "\n",
      "- Consider improving retrieval by refining embeddings or using hybrid retrieval\n",
      "\n",
      "\n",
      "Would you like to save this report to a file? (y/n): y\n",
      "Enter filename (default: rag_evaluation_report.md): rag_evaluation_report.md\n",
      "Report saved to rag_evaluation_report.md\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    print(\"Mock Interviewer App Started\")\n",
    "\n",
    "    # Look for QB.csv file in the current directory/content folder\n",
    "    qb_path = \"/content/processed_data (7).csv\"\n",
    "    if not os.path.exists(qb_path):\n",
    "        print(f\"QB.csv not found in {os.getcwd()}\")\n",
    "        print(\"Looking in /content/ directory...\")\n",
    "        content_path = \"/content/processed_data (7).csv\"\n",
    "        if os.path.exists(content_path):\n",
    "            qb_path = content_path\n",
    "            print(f\"Found QB.csv in /content/ directory\")\n",
    "        else:\n",
    "            print(\"QB.csv not found. Will create a sample question bank.\")\n",
    "\n",
    "    # Initialize the interviewer\n",
    "    interviewer = MockInterviewer(qb_path)\n",
    "\n",
    "    # Get user profile\n",
    "    interviewer.get_user_profile()\n",
    "\n",
    "    # Ask for number of questions\n",
    "    try:\n",
    "        num_questions = int(input(\"\\nHow many questions would you like in this interview? (recommended: 3-5): \"))\n",
    "        if num_questions <= 0:\n",
    "            print(\"Number must be positive. Using default of 3 questions.\")\n",
    "            num_questions = 3\n",
    "        elif num_questions > 10:\n",
    "            print(\"Maximum 10 questions allowed. Using 10 questions.\")\n",
    "            num_questions = 10\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Using default of 3 questions.\")\n",
    "        num_questions = 3\n",
    "\n",
    "    # Conduct the interview\n",
    "    interviewer.conduct_interview(num_questions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5IbtOq6mSMN1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
